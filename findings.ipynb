{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mega cool comparison of the classification to red-wine-quality-competition\n",
    "https://lms.fh-kiel.de/mod/forum/discuss.php?d=54396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.under_sampling import *\n",
    "\n",
    "from imblearn.under_sampling import AllKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datashape: (1599, 11)\n"
     ]
    }
   ],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "df = pd.read_csv(\"winequality-red.csv\")\n",
    "y_tmp = np.array(df[[\"quality\"]])\n",
    "y = []\n",
    "for u in range(0,np.shape(y_tmp)[0]):\n",
    "    #print(y[u])\n",
    "    if y_tmp[u] < 5:\n",
    "        y =np.concatenate((y, [0]))\n",
    "    elif y_tmp[u] < 7:\n",
    "        y =np.concatenate((y, [1]))\n",
    "    else:\n",
    "        #print(\"=\"*5)\n",
    "        y =np.concatenate((y, [2]))\n",
    "df2 = df.drop([\"quality\"], axis=1)\n",
    "X = np.array(df2)\n",
    "print(\"Original datashape: {}\".format(np.shape(X)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "SMOTE\n",
      "Upsampled datashape: (4086, 11)\n",
      "****Results****\n",
      "Accuracy: 83.6186%\n",
      "Log Loss: 0.9856740560791613\n",
      "==========\n",
      "SVMSMOTE\n",
      "Upsampled datashape: (3179, 11)\n",
      "****Results****\n",
      "Accuracy: 83.0189%\n",
      "Log Loss: 1.1428043943845396\n",
      "==========\n",
      "RandomOverSampler\n",
      "Upsampled datashape: (4086, 11)\n",
      "****Results****\n",
      "Accuracy: 88.2641%\n",
      "Log Loss: 0.7840082259221403\n",
      "==========\n",
      "BorderlineSMOTE\n",
      "Upsampled datashape: (4086, 11)\n",
      "****Results****\n",
      "Accuracy: 85.3301%\n",
      "Log Loss: 0.8019545678689887\n"
     ]
    }
   ],
   "source": [
    "sampler = [SMOTE(), SVMSMOTE(), RandomOverSampler(), BorderlineSMOTE()]\n",
    "\n",
    "for UPorDWNsampler in sampler:\n",
    "    print(\"=\"*10)\n",
    "    print(UPorDWNsampler.__class__.__name__)\n",
    "    Xres, yres = UPorDWNsampler.fit_resample(X, y)\n",
    "    print(\"Upsampled datashape: {}\".format(np.shape(Xres)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xres, yres, test_size=0.1, random_state=42)\n",
    "    asd = MLPClassifier(max_iter=750 ,solver='lbfgs', activation='relu', alpha=1, hidden_layer_sizes=(128, 16), random_state=1, warm_start=False, verbose=True)\n",
    "    asd.fit(X_train, y_train)\n",
    "    print('****Results****')\n",
    "    train_predictions = asd.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    train_predictions = asd.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovs = RandomOverSampler()\n",
    "Xres, yres = ovs.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split( Xres, yres, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird ein Gridsearch fÃ¼r die besten Paramester des MLP gemacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 mit 0.878434265010352, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 4 mit 0.8566740609287194, bei folgenden parametern: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (128, 16), 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 2 mit 0.876800133096717, bei folgenden parametern: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 5 mit 0.8558618012422359, bei folgenden parametern: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (128, 16), 'solver': 'adam', 'warm_start': True}\n",
      "Rank 3 mit 0.8672833481218575, bei folgenden parametern: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (128, 16), 'solver': 'lbfgs', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver':('adam', \"sgd\", \"lbfgs\"), \n",
    "              'activation':('identity', 'logistic', 'tanh', 'relu'),\n",
    "              'hidden_layer_sizes':[(128, 16)], \n",
    "              #'learning_rate':('constant', 'invscaling', 'adaptive'),\n",
    "              'alpha':(1, 0.0001),\n",
    "              'warm_start':[True]}\n",
    "mlp =  MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for place, tscore, params in zip(clf.cv_results_['rank_test_score'], clf.cv_results_['mean_test_score'], clf.cv_results_['params']):\n",
    "    if place <= 5:\n",
    "        print(\"Rank {} mit {}, bei folgenden parametern: {}\".format(place, tscore, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 mit 0.8912163561076605, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 5 mit 0.8806048506359065, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'max_iter': 250, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 3 mit 0.8838723750369712, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 2 mit 0.8868655723158829, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'max_iter': 750, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 4 mit 0.8816943951493641, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'max_iter': 1000, 'solver': 'lbfgs', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver':['lbfgs'], \n",
    "              'activation':['tanh'],\n",
    "              'hidden_layer_sizes':[(128, 16)], \n",
    "              #'learning_rate':['adaptive'],\n",
    "              'alpha':[1],\n",
    "              'warm_start':[True],\n",
    "            'max_iter':[200,250,500,750, 1000]}\n",
    "mlp =  MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for place, tscore, params in zip(clf.cv_results_['rank_test_score'], clf.cv_results_['mean_test_score'], clf.cv_results_['params']):\n",
    "    if place <= 5:\n",
    "        print(\"Rank {} mit {}, bei folgenden parametern: {}\".format(place, tscore, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 mit 0.7908625406684413, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 2), 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 4 mit 0.8501519520851819, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 8), 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 3 mit 0.8637503697131027, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 8), 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 1 mit 0.8855039189588879, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 2 mit 0.8844191807157646, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (256, 16), 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver':['lbfgs'], \n",
    "              'activation':['tanh'],\n",
    "              'hidden_layer_sizes':[(32, 2), (32, 8),(128, 8), (128, 16), (256, 16)], \n",
    "              #'learning_rate':['adaptive'],\n",
    "              'alpha':[1],\n",
    "              'warm_start':[True],\n",
    "            'max_iter':[200]}\n",
    "mlp =  MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for place, tscore, params in zip(clf.cv_results_['rank_test_score'], clf.cv_results_['mean_test_score'], clf.cv_results_['params']):\n",
    "    if place <= 5:\n",
    "        print(\"Rank {} mit {}, bei folgenden parametern: {}\".format(place, tscore, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Results****\n",
      "Accuracy: 86.3081%\n",
      "Log Loss: 0.4900513733239912\n"
     ]
    }
   ],
   "source": [
    "asd = MLPClassifier(max_iter=200 ,\n",
    "                    solver='lbfgs', \n",
    "                    activation='tanh', \n",
    "                    learning_rate=\"adaptive\", \n",
    "                    alpha=1, \n",
    "                    hidden_layer_sizes=(128, 16), \n",
    "                    random_state=1, warm_start=False, verbose=True)\n",
    "asd.fit(X_train, y_train)\n",
    "print('****Results****')\n",
    "train_predictions = asd.predict(X_test)\n",
    "acc = accuracy_score(y_test, train_predictions)\n",
    "print(\"Accuracy: {:.4%}\".format(acc))\n",
    "train_predictions = asd.predict_proba(X_test)\n",
    "ll = log_loss(y_test, train_predictions)\n",
    "print(\"Log Loss: {}\".format(ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Accuracy: 93.9394%\n",
      "Log Loss: 1.01083453716749\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 66.4141%\n",
      "Log Loss: 0.7349104346135595\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 0.002596915960816789\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 0.005212715195134082\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Accuracy: 76.5152%\n",
      "Log Loss: 0.5335394187198177\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 78.2828%\n",
      "Log Loss: 0.6258252277995481\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Accuracy: 97.7273%\n",
      "Log Loss: 0.07982780923179084\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Accuracy: 95.2020%\n",
      "Log Loss: 0.22680281400958213\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Accuracy: 83.5859%\n",
      "Log Loss: 0.43161081324683653\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 71.2121%\n",
      "Log Loss: 0.9969069704808532\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 65.1515%\n",
      "Log Loss: 1.125465129702411\n",
      "==============================\n",
      "QuadraticDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 70.9596%\n",
      "Log Loss: 0.8200447519117497\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X_train, y_train)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "    SVC(gamma=2, C=1, probability=True),\n",
    "    SVC(C=1000, gamma=1, kernel='rbf', probability=True),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(max_iter=750 ,solver='lbfgs', activation='relu', alpha=1, hidden_layer_sizes=(128, 16), random_state=1, warm_start=False, verbose=True),\n",
    "    MLPClassifier(max_iter=200 ,solver='lbfgs',activation='tanh', learning_rate=\"adaptive\",alpha=1, hidden_layer_sizes=(128, 16), \n",
    "                    random_state=1, warm_start=False, verbose=True),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch for alternatic SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.899 (+/-0.010) for {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.757 (+/-0.024) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.469 (+/-0.034) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.315 (+/-0.038) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.904 (+/-0.016) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.843 (+/-0.016) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.527 (+/-0.041) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.469 (+/-0.031) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.904 (+/-0.015) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.868 (+/-0.020) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.598 (+/-0.037) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.510 (+/-0.038) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.904 (+/-0.015) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.870 (+/-0.020) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.673 (+/-0.039) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.568 (+/-0.037) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.558 (+/-0.030) for {'C': 1, 'kernel': 'linear'}\n",
      "0.553 (+/-0.035) for {'C': 10, 'kernel': 'linear'}\n",
      "0.553 (+/-0.035) for {'C': 100, 'kernel': 'linear'}\n",
      "0.555 (+/-0.038) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00        58\n",
      "           4       1.00      1.00      1.00        71\n",
      "           5       0.68      0.78      0.73        72\n",
      "           6       0.79      0.65      0.71        84\n",
      "           7       0.92      0.98      0.95        58\n",
      "           8       1.00      1.00      1.00        66\n",
      "\n",
      "    accuracy                           0.89       409\n",
      "   macro avg       0.90      0.90      0.90       409\n",
      "weighted avg       0.89      0.89      0.89       409\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.899 (+/-0.012) for {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.769 (+/-0.023) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.483 (+/-0.033) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.325 (+/-0.022) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.905 (+/-0.015) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.850 (+/-0.013) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.546 (+/-0.035) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.486 (+/-0.031) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.905 (+/-0.014) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.874 (+/-0.017) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.612 (+/-0.028) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.526 (+/-0.033) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.905 (+/-0.014) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.876 (+/-0.018) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.684 (+/-0.038) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.587 (+/-0.034) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.579 (+/-0.027) for {'C': 1, 'kernel': 'linear'}\n",
      "0.572 (+/-0.030) for {'C': 10, 'kernel': 'linear'}\n",
      "0.572 (+/-0.030) for {'C': 100, 'kernel': 'linear'}\n",
      "0.573 (+/-0.032) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00        58\n",
      "           4       1.00      1.00      1.00        71\n",
      "           5       0.70      0.79      0.75        72\n",
      "           6       0.80      0.68      0.74        84\n",
      "           7       0.92      0.98      0.95        58\n",
      "           8       1.00      1.00      1.00        66\n",
      "\n",
      "    accuracy                           0.89       409\n",
      "   macro avg       0.90      0.91      0.91       409\n",
      "weighted avg       0.90      0.89      0.89       409\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 0.1, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    #{'kernel': ['poly'], 'C': [1, 10, 100, 1000], 'degree':[1,2,5,10,15,20], 'coef0':[0.001, 0.01, 0.1, 1]}\n",
    "                   ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
