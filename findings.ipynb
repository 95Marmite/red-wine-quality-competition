{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mega cool comparison of the classification to red-wine-quality-competition\n",
    "https://lms.fh-kiel.de/mod/forum/discuss.php?d=54396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from imblearn.under_sampling import AllKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datashape: (1599, 11)\n"
     ]
    }
   ],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "df = pd.read_csv(\"Datasets/winequality-red.csv\")\n",
    "y = np.array(df[[\"quality\"]])\n",
    "df2 = df.drop([\"quality\"], axis=1)\n",
    "X = np.array(df2)\n",
    "print(\"Original datashape: {}\".format(np.shape(X)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "SMOTE\n",
      "Upsampled datashape: (4086, 11)\n",
      "****Results****\n",
      "Accuracy: 83.6186%\n",
      "Log Loss: 0.9856740560791613\n",
      "==========\n",
      "SVMSMOTE\n",
      "Upsampled datashape: (3179, 11)\n",
      "****Results****\n",
      "Accuracy: 83.0189%\n",
      "Log Loss: 1.1428043943845396\n",
      "==========\n",
      "RandomOverSampler\n",
      "Upsampled datashape: (4086, 11)\n",
      "****Results****\n",
      "Accuracy: 88.2641%\n",
      "Log Loss: 0.7840082259221403\n",
      "==========\n",
      "BorderlineSMOTE\n",
      "Upsampled datashape: (4086, 11)\n",
      "****Results****\n",
      "Accuracy: 85.3301%\n",
      "Log Loss: 0.8019545678689887\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import *\n",
    "from imblearn.under_sampling import *\n",
    "sampler = [SMOTE(), SVMSMOTE(), RandomOverSampler(), BorderlineSMOTE()]\n",
    "\n",
    "for UPorDWNsampler in sampler:\n",
    "    print(\"=\"*10)\n",
    "    print(UPorDWNsampler.__class__.__name__)\n",
    "    Xres, yres = UPorDWNsampler.fit_resample(X, y)\n",
    "    print(\"Upsampled datashape: {}\".format(np.shape(Xres)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xres, yres, test_size=0.1, random_state=42)\n",
    "    asd = MLPClassifier(max_iter=750 ,solver='lbfgs', activation='relu', alpha=1, hidden_layer_sizes=(128, 16), random_state=1, warm_start=False, verbose=True)\n",
    "    asd.fit(X_train, y_train)\n",
    "    print('****Results****')\n",
    "    train_predictions = asd.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    train_predictions = asd.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovs = RandomOverSampler()\n",
    "Xres, yres = UPorDWNsampler.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split( Xres, yres, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird ein Gridsearch f√ºr die besten Paramester des MLP gemacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 mit 0.8659228039041704, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'constant', 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 2 mit 0.8735314995563442, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'invscaling', 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 1 mit 0.8746221532091096, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 3 mit 0.8675528689736763, bei folgenden parametern: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'constant', 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 4 mit 0.8659276101745045, bei folgenden parametern: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'solver': 'lbfgs', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver':('adam', \"sgd\", \"lbfgs\"), \n",
    "              'activation':('identity', 'logistic', 'tanh', 'relu'),\n",
    "              'hidden_layer_sizes':[(128, 16)], \n",
    "              'learning_rate':('constant', 'invscaling', 'adaptive'),\n",
    "              'alpha':(1, 0.0001),\n",
    "              'warm_start':[True]}\n",
    "mlp =  MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for place, tscore, params in zip(clf.cv_results_['rank_test_score'], clf.cv_results_['mean_test_score'], clf.cv_results_['params']):\n",
    "    if place <= 5:\n",
    "        print(\"Rank {} mit {}, bei folgenden parametern: {}\".format(place, tscore, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 mit 0.8642897811298432, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 1 mit 0.8740823720792665, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'max_iter': 250, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 4 mit 0.8697286305826679, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 3 mit 0.8729924578527063, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'max_iter': 750, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 2 mit 0.8732608695652175, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'lbfgs', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver':['lbfgs'], \n",
    "              'activation':['tanh'],\n",
    "              'hidden_layer_sizes':[(128, 16)], \n",
    "              'learning_rate':['adaptive'],\n",
    "              'alpha':[1],\n",
    "              'warm_start':[True],\n",
    "            'max_iter':[200,250,500,750, 1000]}\n",
    "mlp =  MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for place, tscore, params in zip(clf.cv_results_['rank_test_score'], clf.cv_results_['mean_test_score'], clf.cv_results_['params']):\n",
    "    if place <= 5:\n",
    "        print(\"Rank {} mit {}, bei folgenden parametern: {}\".format(place, tscore, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 mit 0.8112570245489501, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 2), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 4 mit 0.8542280390417037, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 8), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 1 mit 0.8738135906536527, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 8), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 3 mit 0.8686446317657497, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 16), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Rank 2 mit 0.869460958296362, bei folgenden parametern: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (256, 16), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'lbfgs', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver':['lbfgs'], \n",
    "              'activation':['tanh'],\n",
    "              'hidden_layer_sizes':[(32, 2), (32, 8),(128, 8), (128, 16), (256, 16)], \n",
    "              'learning_rate':['adaptive'],\n",
    "              'alpha':[1],\n",
    "              'warm_start':[True],\n",
    "            'max_iter':[500]}\n",
    "mlp =  MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for place, tscore, params in zip(clf.cv_results_['rank_test_score'], clf.cv_results_['mean_test_score'], clf.cv_results_['params']):\n",
    "    if place <= 5:\n",
    "        print(\"Rank {} mit {}, bei folgenden parametern: {}\".format(place, tscore, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Results****\n",
      "Accuracy: 85.5746%\n",
      "Log Loss: 0.7754605730017772\n"
     ]
    }
   ],
   "source": [
    "asd = MLPClassifier(max_iter=750 ,\n",
    "                    solver='lbfgs', \n",
    "                    activation='tanh', \n",
    "                    learning_rate=\"adaptive\", \n",
    "                    alpha=1, \n",
    "                    hidden_layer_sizes=(128, 8), \n",
    "                    random_state=1, warm_start=False, verbose=True)\n",
    "asd.fit(X_train, y_train)\n",
    "print('****Results****')\n",
    "train_predictions = asd.predict(X_test)\n",
    "acc = accuracy_score(y_test, train_predictions)\n",
    "print(\"Accuracy: {:.4%}\".format(acc))\n",
    "train_predictions = asd.predict_proba(X_test)\n",
    "ll = log_loss(y_test, train_predictions)\n",
    "print(\"Log Loss: {}\".format(ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Accuracy: 80.9291%\n",
      "Log Loss: 3.285927220129726\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 64.0587%\n",
      "Log Loss: 0.8713212111428007\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 86.5526%\n",
      "Log Loss: 0.30842294181014135\n",
      "==============================\n",
      "GaussianProcessClassifier\n",
      "****Results****\n",
      "Accuracy: 85.5746%\n",
      "Log Loss: 0.9091892121486286\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Accuracy: 61.6137%\n",
      "Log Loss: 1.074955746556195\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 71.1491%\n",
      "Log Loss: 0.9215940155193033\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Accuracy: 75.0611%\n",
      "Log Loss: 0.6708703867779559\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 42.0538%\n",
      "Log Loss: 2.021383662597486\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 58.9242%\n",
      "Log Loss: 1.4417142160583905\n",
      "==============================\n",
      "QuadraticDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 72.1271%\n",
      "Log Loss: 0.9633711180863739\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X_train, y_train)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "    SVC(gamma=2, C=1, probability=True),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch for alternatic SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.888 (+/-0.005) for {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.784 (+/-0.022) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.609 (+/-0.082) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.348 (+/-0.023) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.893 (+/-0.011) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.839 (+/-0.022) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.652 (+/-0.048) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.611 (+/-0.079) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.894 (+/-0.012) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.858 (+/-0.017) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.688 (+/-0.046) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.652 (+/-0.047) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.894 (+/-0.012) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.017) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.736 (+/-0.037) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.680 (+/-0.046) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.685 (+/-0.039) for {'C': 1, 'kernel': 'linear'}\n",
      "0.688 (+/-0.035) for {'C': 10, 'kernel': 'linear'}\n",
      "0.689 (+/-0.038) for {'C': 100, 'kernel': 'linear'}\n",
      "0.689 (+/-0.037) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-927e75f78f8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 0.1, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    #{'kernel': ['poly'], 'C': [1, 10, 100, 1000], 'degree':[1,2,5,10,15,20], 'coef0':[0.001, 0.01, 0.1, 1]}\n",
    "                   ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
