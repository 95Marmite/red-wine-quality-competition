{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from imblearn.over_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "Original datashape: (1599, 11)\n",
      "Original datashape: (1599, 1)\n",
      "1319\n",
      "1319\n",
      "1319\n",
      "0\n",
      "Upsampled datashape: (3957, 11)\n",
      "Upsampled datashape: (3957,)\n",
      "[1. 1. 1. ... 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"winequality-red.csv\")\n",
    "y_tmp = np.array(df[[\"quality\"]])\n",
    "y = []\n",
    "for u in range(0,np.shape(y_tmp)[0]):\n",
    "    #print(y[u])\n",
    "    if y_tmp[u] < 5:\n",
    "        y =np.concatenate((y, [0]))\n",
    "    elif y_tmp[u] < 7:\n",
    "        y =np.concatenate((y, [1]))\n",
    "    else:\n",
    "        #print(\"=\"*5)\n",
    "        y =np.concatenate((y, [2]))\n",
    "        \n",
    "print(y)\n",
    "        \n",
    "df2 = df.drop([\"quality\"], axis=1)\n",
    "X = np.array(df2)\n",
    "print(\"Original datashape: {}\".format(np.shape(X)))\n",
    "print(\"Original datashape: {}\".format(np.shape(y_tmp)))\n",
    "\n",
    "ovs = RandomOverSampler()\n",
    "Xres, yres = ovs.fit_resample(X, y)\n",
    "\n",
    "good = 0\n",
    "mid = 0\n",
    "bad = 0\n",
    "rest = 0\n",
    "for e in yres:\n",
    "    if e == 0:\n",
    "        bad = bad + 1\n",
    "    elif e == 1:\n",
    "        mid = mid +1\n",
    "    elif e == 2:\n",
    "        good = good +1\n",
    "    else:\n",
    "        print(e)\n",
    "        rest = rest +1\n",
    "print(good)\n",
    "print(mid)\n",
    "print(bad)\n",
    "print(rest)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xres)\n",
    "Xres = scaler.transform(Xres)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( Xres, yres, test_size=0.1, random_state=42)\n",
    "y_train = to_categorical(y_train)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size=0.01, random_state=42)\n",
    "print(\"Upsampled datashape: {}\".format(np.shape(Xres)))\n",
    "print(\"Upsampled datashape: {}\".format(np.shape(yres)))\n",
    "print(yres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(11, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(64, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"layer3\"),\n",
    "        layers.Dense(32, activation=\"relu\", name=\"layer4\"),\n",
    "        layers.Dense(3, activation=\"softmax\" , name=\"out\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3525 samples, validate on 36 samples\n",
      "Epoch 1/50\n",
      "3525/3525 [==============================] - 1s 349us/sample - loss: 0.6905 - val_loss: 0.8894\n",
      "Epoch 2/50\n",
      "3525/3525 [==============================] - 1s 234us/sample - loss: 0.5192 - val_loss: 0.8023\n",
      "Epoch 3/50\n",
      "3525/3525 [==============================] - 1s 239us/sample - loss: 0.4517 - val_loss: 0.5898\n",
      "Epoch 4/50\n",
      "3525/3525 [==============================] - 1s 229us/sample - loss: 0.3920 - val_loss: 0.5717\n",
      "Epoch 5/50\n",
      "3525/3525 [==============================] - 1s 192us/sample - loss: 0.3741 - val_loss: 0.5719\n",
      "Epoch 6/50\n",
      "3525/3525 [==============================] - 1s 204us/sample - loss: 0.3415 - val_loss: 0.5182\n",
      "Epoch 7/50\n",
      "3525/3525 [==============================] - 1s 242us/sample - loss: 0.3148 - val_loss: 0.4575\n",
      "Epoch 8/50\n",
      "3525/3525 [==============================] - 1s 274us/sample - loss: 0.3043 - val_loss: 0.4318\n",
      "Epoch 9/50\n",
      "3525/3525 [==============================] - 1s 223us/sample - loss: 0.2632 - val_loss: 0.4168\n",
      "Epoch 10/50\n",
      "3525/3525 [==============================] - 1s 224us/sample - loss: 0.2438 - val_loss: 0.3767\n",
      "Epoch 11/50\n",
      "3525/3525 [==============================] - 1s 229us/sample - loss: 0.2446 - val_loss: 0.4328\n",
      "Epoch 12/50\n",
      "3525/3525 [==============================] - 1s 225us/sample - loss: 0.2141 - val_loss: 0.4068\n",
      "Epoch 13/50\n",
      "3525/3525 [==============================] - 1s 233us/sample - loss: 0.2005 - val_loss: 0.3804\n",
      "Epoch 14/50\n",
      "3525/3525 [==============================] - 1s 217us/sample - loss: 0.2000 - val_loss: 0.4440\n",
      "Epoch 15/50\n",
      "3525/3525 [==============================] - 1s 230us/sample - loss: 0.1790 - val_loss: 0.3939\n",
      "Epoch 16/50\n",
      "3525/3525 [==============================] - 1s 231us/sample - loss: 0.1688 - val_loss: 0.2579\n",
      "Epoch 17/50\n",
      "3525/3525 [==============================] - 1s 236us/sample - loss: 0.1589 - val_loss: 0.5117\n",
      "Epoch 18/50\n",
      "3525/3525 [==============================] - 1s 233us/sample - loss: 0.1513 - val_loss: 0.3226\n",
      "Epoch 19/50\n",
      "3525/3525 [==============================] - 1s 231us/sample - loss: 0.1478 - val_loss: 0.5584\n",
      "Epoch 20/50\n",
      "3525/3525 [==============================] - 1s 230us/sample - loss: 0.1492 - val_loss: 0.3614\n",
      "Epoch 21/50\n",
      "3525/3525 [==============================] - 1s 227us/sample - loss: 0.1237 - val_loss: 0.3442\n",
      "Epoch 22/50\n",
      "3525/3525 [==============================] - 1s 232us/sample - loss: 0.1212 - val_loss: 0.5160\n",
      "Epoch 23/50\n",
      "3525/3525 [==============================] - 1s 228us/sample - loss: 0.1299 - val_loss: 0.3432\n",
      "Epoch 24/50\n",
      "3525/3525 [==============================] - 1s 238us/sample - loss: 0.1049 - val_loss: 0.3354\n",
      "Epoch 25/50\n",
      "3525/3525 [==============================] - 1s 229us/sample - loss: 0.1399 - val_loss: 0.2861\n",
      "Epoch 26/50\n",
      "3525/3525 [==============================] - 1s 223us/sample - loss: 0.0944 - val_loss: 0.3003\n",
      "Epoch 27/50\n",
      "3525/3525 [==============================] - 1s 262us/sample - loss: 0.0907 - val_loss: 0.3191\n",
      "Epoch 28/50\n",
      "3525/3525 [==============================] - 1s 262us/sample - loss: 0.1029 - val_loss: 0.3739\n",
      "Epoch 29/50\n",
      "3525/3525 [==============================] - 1s 233us/sample - loss: 0.1285 - val_loss: 0.4132\n",
      "Epoch 30/50\n",
      "3525/3525 [==============================] - 1s 238us/sample - loss: 0.0921 - val_loss: 0.3791\n",
      "Epoch 31/50\n",
      "3525/3525 [==============================] - 1s 232us/sample - loss: 0.0636 - val_loss: 0.3770\n",
      "Epoch 32/50\n",
      "3525/3525 [==============================] - 1s 230us/sample - loss: 0.0755 - val_loss: 0.7034\n",
      "Epoch 33/50\n",
      "3525/3525 [==============================] - 1s 228us/sample - loss: 0.1013 - val_loss: 0.4191\n",
      "Epoch 34/50\n",
      "3525/3525 [==============================] - 1s 227us/sample - loss: 0.0620 - val_loss: 0.2980\n",
      "Epoch 35/50\n",
      "3525/3525 [==============================] - 1s 233us/sample - loss: 0.0702 - val_loss: 0.2536\n",
      "Epoch 36/50\n",
      "3525/3525 [==============================] - 1s 235us/sample - loss: 0.0627 - val_loss: 0.4508\n",
      "Epoch 37/50\n",
      "3525/3525 [==============================] - 1s 228us/sample - loss: 0.0852 - val_loss: 0.4961\n",
      "Epoch 38/50\n",
      "3525/3525 [==============================] - 1s 231us/sample - loss: 0.0853 - val_loss: 0.7863\n",
      "Epoch 39/50\n",
      "3525/3525 [==============================] - 1s 227us/sample - loss: 0.0631 - val_loss: 0.5585\n",
      "Epoch 40/50\n",
      "3525/3525 [==============================] - 1s 226us/sample - loss: 0.0692 - val_loss: 0.2576\n",
      "Epoch 41/50\n",
      "3525/3525 [==============================] - 1s 230us/sample - loss: 0.0522 - val_loss: 0.2978\n",
      "Epoch 42/50\n",
      "3525/3525 [==============================] - 1s 223us/sample - loss: 0.0681 - val_loss: 0.5909\n",
      "Epoch 43/50\n",
      "3525/3525 [==============================] - 1s 226us/sample - loss: 0.0558 - val_loss: 0.7342\n",
      "Epoch 44/50\n",
      "3525/3525 [==============================] - 1s 237us/sample - loss: 0.0601 - val_loss: 0.6976\n",
      "Epoch 45/50\n",
      "3525/3525 [==============================] - 1s 228us/sample - loss: 0.0715 - val_loss: 0.2785\n",
      "Epoch 46/50\n",
      "3525/3525 [==============================] - 1s 239us/sample - loss: 0.0518 - val_loss: 0.4363\n",
      "Epoch 47/50\n",
      "3525/3525 [==============================] - 1s 268us/sample - loss: 0.0640 - val_loss: 0.5594\n",
      "Epoch 48/50\n",
      "3525/3525 [==============================] - 1s 235us/sample - loss: 0.0353 - val_loss: 0.5570\n",
      "Epoch 49/50\n",
      "3525/3525 [==============================] - 1s 225us/sample - loss: 0.0394 - val_loss: 0.5043\n",
      "Epoch 50/50\n",
      "3525/3525 [==============================] - 1s 231us/sample - loss: 0.0678 - val_loss: 0.4357\n",
      "\n",
      "history dict: {'loss': [0.6904674563741853, 0.5191737203733295, 0.45166600016309016, 0.39202040468890825, 0.3740633284105054, 0.3415434238811334, 0.3147639615342338, 0.3043030037987211, 0.26322455241853465, 0.2438401774040932, 0.24463842435317207, 0.21406566984381803, 0.20049933515660834, 0.1999779871493208, 0.1789980037788471, 0.1687699117133863, 0.1588576921351842, 0.15129652938121063, 0.1477805038871151, 0.1492166139896821, 0.12373015562853452, 0.12119228238746002, 0.1298616365965535, 0.10490137020176349, 0.1398715134556272, 0.09435796424352683, 0.0906710085624775, 0.10289244862299755, 0.12854734095602366, 0.09205600857865338, 0.06359249417923336, 0.0755069433540768, 0.10128138447167619, 0.062021629446674185, 0.07023978269863225, 0.0626640856098778, 0.08523625828575143, 0.08530872168743152, 0.06306262223607512, 0.06924174939750041, 0.05220197248133029, 0.06814644733736869, 0.05583675470343546, 0.06007799290989189, 0.07153421784106359, 0.05177847345203836, 0.06404500365814983, 0.035272740000608156, 0.03939203298445884, 0.0677941389368714], 'val_loss': [0.8894040129250951, 0.8023160174489021, 0.5897569962673717, 0.5716996050129334, 0.5718869202666812, 0.5181523391769992, 0.45751725509762764, 0.4318401414073176, 0.4168429244309664, 0.37668697256594896, 0.43279696131745976, 0.40676042903214693, 0.3804485070415669, 0.44404098557101357, 0.3938692110694117, 0.2579335152792434, 0.5117062898870144, 0.32262715738680625, 0.5583584409517547, 0.36144581504373086, 0.34416163547171486, 0.5160057590756979, 0.3431832624547597, 0.3354352231544908, 0.28606045737979, 0.3002608604923201, 0.3191370254692932, 0.3738753516162332, 0.4132157183226405, 0.37908780028940076, 0.3769924939325493, 0.7033794446587207, 0.4190725776530194, 0.2980131223763844, 0.2535578464982488, 0.45075567204427597, 0.49606470967450556, 0.7863199572392559, 0.5585096153032383, 0.25757993894416764, 0.2978199160526047, 0.5909280716413277, 0.734166858265174, 0.6975734523099769, 0.2785198865054149, 0.4363290555096076, 0.5593917377781408, 0.5570007652083101, 0.5042731431640277, 0.4356818108741815]}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=5,\n",
    "                    epochs=50,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(X_val, y_val)\n",
    "                   )\n",
    "\n",
    "print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.2020%\n"
     ]
    }
   ],
   "source": [
    "train_predictions=model.predict(X_test)\n",
    "y_pred = []\n",
    "for t in train_predictions:\n",
    "    y_pred.append(np.argmax(t))\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy: {:.4%}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
